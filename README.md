# Neural Network from Scratch

A simple yet powerful feedforward neural network built from scratch using only **NumPy**. This project trains on the **MNIST dataset** (handwritten digits) and can also classify custom **28Ã—28 images**.

---

## ğŸ“Œ Table of Contents

- [ğŸ“– Overview](#-overview)
- [ğŸ“ Project Structure](#-project-structure)
- [âš™ï¸ Installation](#-installation)
- [ğŸš€ Usage](#-usage)
  - [1ï¸âƒ£ Training the Model](#1ï¸âƒ£-training-the-model)
  - [2ï¸âƒ£ Testing on MNIST Test Set](#2ï¸âƒ£-testing-on-mnist-test-set)
  - [3ï¸âƒ£ Testing with Custom Images](#3ï¸âƒ£-testing-with-custom-images)
- [ğŸ§  How It Works](#-how-it-works)
  - [Network Architecture](#network-architecture)
  - [Forward Propagation](#forward-propagation)
  - [Backpropagation](#backpropagation)
  - [Parameter Updates](#parameter-updates)
- [ğŸ“Œ Code Files](#-code-files)
- [ğŸ“œ License](#-license)

---

## ğŸ“– Overview

âœ… **Goal:** Classify handwritten digits (0â€“9) using a neural network built from scratch.
âœ… **Dataset:** [MNIST](http://yann.lecun.com/exdb/mnist/) (60,000 training, 10,000 test images).
âœ… **Testing:** Custom 28Ã—28 **JPEG** digit images stored in the `image/` directory.

ğŸ–¥ï¸ **Neural Network Architecture:**
- **Input Layer:** 784 neurons (one per pixel in a 28Ã—28 image).
- **Hidden Layer:** 64 neurons (configurable) using **ReLU activation**.
- **Output Layer:** 10 neurons (one per digit, 0-9) using **softmax activation**.
- **Loss Function:** Cross-Entropy Loss.
- **Optimizer:** Mini-batch Gradient Descent.

---

## ğŸ“ Project Structure

```
NEURAL_NETWORK/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ test_images/          # Custom test images
â”‚   â”œâ”€â”€ mnist_test.csv        # MNIST test dataset
â”‚   â””â”€â”€ mnist_train.csv       # MNIST training dataset
â”œâ”€â”€ model/
â”‚   â””â”€â”€ nn_mnist_model.pkl    # Saved trained model
â”œâ”€â”€ neural_network/
â”‚   â”œâ”€â”€ nn_core/
â”‚   â”‚   â”œâ”€â”€ backpropagation.py
â”‚   â”‚   â”œâ”€â”€ forward_propagation.py
â”‚   â”‚   â”œâ”€â”€ params_update.py
â”‚   â”‚   â””â”€â”€ train_loop.py
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ nn_class.py
â”‚   â”œâ”€â”€ nn_testing.py
â”‚   â”œâ”€â”€ nn_training.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ nn.ipynb              # Jupyter Notebook
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### ğŸ”¹ Step 1: Clone the Repository
```bash
git clone https://github.com/isatyamks/neural_network.git
cd neural_network
```

### ğŸ”¹ Step 2: Set Up a Virtual Environment (Recommended)
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### ğŸ”¹ Step 3: Install Dependencies
```bash
pip install numpy pandas matplotlib
```
*(Add `Pillow` if working with images.)*

---

## ğŸš€ Usage

### 1ï¸âƒ£ Training the Model
âœ… Ensure MNIST dataset (`mnist_train.csv`, `mnist_test.csv`) is in the `data/` folder.
âœ… Run the training script:
```bash
python train.py
```

ğŸ“Œ **What this does:**
- Loads MNIST data.
- Initializes and trains the neural network.
- Evaluates accuracy on the MNIST test set.
- Saves the trained model to `model/nn_mnist_model.pkl`.
- Plots training and validation loss.

---

### 2ï¸âƒ£ Testing on MNIST Test Set
Once trained, the script will **automatically** evaluate the model on `mnist_test.csv` and print accuracy.

---

### 3ï¸âƒ£ Testing with Custom Images
Want to test on **your own** handwritten digits?

âœ… Save 28Ã—28 **JPEG** images in `image/`.
âœ… Run the script:
```bash
python testing.py
```

ğŸ“Œ **What this does:**
- Loads the trained model (`nn_mnist_model.pkl`).
- Reads each image from `image/`, converts to a NumPy array (28Ã—28).
- Normalizes, flattens, and predicts the digit class.

ğŸ’¡ *Modify `testing.py` to display the image alongside its predicted label!*

---

## ğŸ§  How It Works

### ğŸ”¹ Network Architecture
| Layer          | Details                          |
|---------------|---------------------------------|
| **Input**     | 28Ã—28 = 784 neurons             |
| **Hidden**    | 64 neurons, ReLU activation    |
| **Output**    | 10 neurons, Softmax activation |

---

### ğŸ”¹ Forward Propagation
1ï¸âƒ£ **Compute Weighted Sum:**
   \[ Z = W \times X + b \]
2ï¸âƒ£ **Apply Activation Function:**
   - Hidden Layer: **ReLU** (max(0, Z))
   - Output Layer: **Softmax** (probability distribution over 10 digits)

---

### ğŸ”¹ Backpropagation
1ï¸âƒ£ Compute **Loss** (Cross-Entropy for classification).
2ï¸âƒ£ Calculate **Gradients** using the **Chain Rule**.
3ï¸âƒ£ Update **Weights & Biases** (Mini-batch Gradient Descent):
   \[ W = W - \alpha \times \text{gradient} \]

---

### ğŸ”¹ Parameter Updates
âœ… **Gradient Descent** minimizes loss by adjusting weights using `params_update.py`.
âœ… Adjust **learning rate** and **batch size** for better performance.

---

## ğŸ“Œ Code Files

ğŸ”¹ **Neural Network Class:** [`nn_class.py`](#)
ğŸ”¹ **Training Script:** [`nn_training.py`](#)
ğŸ”¹ **Testing Script:** [`nn_testing.py`](#)
ğŸ”¹ **Data Preprocessing:** [`data_preprocessing.py`](#)
ğŸ”¹ **Core Modules (Forward, Backward, Update):** [`nn_core/`](#)

---

## ğŸ“œ License
This project is open-source under the **MIT License**. Feel free to use, modify, and improve it!

ğŸš€ **Happy Learning & Coding!** ğŸ’¡

